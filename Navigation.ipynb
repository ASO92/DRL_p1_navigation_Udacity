{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/).There are other packages necessary to import also that sholud be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C:/Users/rovaa/deep-reinforcement-learning/p1_navigation/Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0] # Return BananaBrain\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "The simulation contains a single agent that navigates a large environment. At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the agent is performing 2 straigth movemenens and 2 left turn movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0 \n",
    "n = 20\n",
    "tramo = 5\n",
    "for i in range(n): # n steps\n",
    "    # print(i)\n",
    "    if i < tramo:\n",
    "        action = 0       # select walk forward.\n",
    "    elif tramo <= i< 2*tramo:\n",
    "        action = 2\n",
    "    elif 2*tramo<= i< 3*tramo:\n",
    "        action = 0\n",
    "    elif 3*tramo<= i< 4*tramo:\n",
    "        action = 2\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    #print(done)\n",
    "    score += reward                                # update the score\n",
    " \n",
    "print(\"Score: {}\".format(score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell the agent is performing random actions until the episode finishes. It returns the score in the end of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action randomly.\n",
    "    #The randint() method returns an integer number selected element from the specified range. Si solo meto un argumento, es es el valor mÃ¡ximo\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the DQN agent. \n",
    "The following dqn function is intended to follow de implementation schema of a DQN agent.\n",
    "\n",
    "<img src=\"assets/DQN_algorithm.png\" width=\"40%\" align=\"left\" alt=\"\" title=\"Optimal Policy Equation\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "def dqn(n_episodes=200, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, \n",
    "        train_mode = True, ckpt_path='pth_checkpoints/checkpoint.pth',target_stop = True, save_weights= True):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        train_mode(bool): if 'True' set environment to training mode\n",
    "        ckt_path(string): it is the path to set the weights of the trained \n",
    "        episode_Stop(bool): if True, the simulation stops when target is reached\n",
    "        save_weights(bool): if 'True' save weights of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    moving_avgs = [] \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # reset the environment\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]\n",
    "        state = env_info.vector_observations[0]  ##env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            action = action.astype(int) # Important acions must be int32 type.\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score of an episode to scores_window, the window for the moving average a deque of 100\n",
    "        scores.append(score)              # save most recent score of an episode to total scores, total amount of scores\n",
    "        moving_avg = np.mean(scores_window)  # calculate moving average with the scores window\n",
    "        moving_avgs.append(moving_avg)       # save most recent moving average\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, moving_avg), end=\"\")\n",
    "        #if i_episode % 10 == 0 and i_episode<100:\n",
    "        #    print('\\r Saving weights at episode {}'.format(i_episode))\n",
    "        #    torch.save(agent.qnetwork_local.state_dict(), 'checkpoint{}.pth'.format(i_episode))\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, moving_avg))\n",
    "            #print('\\r Saving weights at episode {}'.format(i_episode))\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'checkpoint{}.pth'.format(i_episode))\n",
    "        if target_stop:\n",
    "            if moving_avg>=13.0:\n",
    "                # The task is episodic, and in order to solve the environment, \n",
    "                # your agent must get an average score of +13 over 100 consecutive episodes. \n",
    "                # Eso quiere decir que el agente estÃ¡ entrenado en los i-100 episodios,\n",
    "                # porque los Ãºltimos 100 han sido test para calcular su average score  \n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, moving_avg))\n",
    "                if save_weights:\n",
    "                    torch.save(agent.qnetwork_local.state_dict(), pth_checkpoints)\n",
    "                break\n",
    "    return scores, moving_avgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run different tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Standard case with standard values\n",
    "In this case it is intended to obtain the number of training episodes necessary to get the 13 average score in the last 100 episodes. Weights of the agent trained are also saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1\tAverage Score: -3.00"
     ]
    }
   ],
   "source": [
    "# Create the agent\n",
    "agent = Agent(state_size= 37, action_size=4, seed=0, dueling=False, double=False)\n",
    "\n",
    "# Train the agent\n",
    "start_time = time.time() # Monitor Training Time  \n",
    "scores,avgs = dqn(n_episodes=10, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, \n",
    "        train_mode = True, ckpt_path='pth_checkpoints/checkpoint.pth', save_weights = True) # standard values\n",
    "print(\"\\nTotal Training time = {:.1f} min\".format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the trained agent evolution, considering the scores per episode and de average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.plot(np.arange(len(scores)), scores, label='prueba1')\n",
    "plt.plot(np.arange(len(scores)), avgs_1, color='blue', label='average')\n",
    "plt.plot(np.arange(len(scores)), avgs_2, color='lightblue', label='average')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "print(\"Score: {}\".format(scores)) # Sacar la secuencia de score de los Ãºltimos 100 episodios, sobre los que se hace la media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Average sensibility to eps_decay for standard case test\n",
    "In this test it is intended to see the performance evolution of the algorithm in terms of changin the eps_decay, and consecuently going to a transition of exploration to explotation much faster in each of the eps_decays values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs=[]\n",
    "scores=[]\n",
    "eps_decay=[0.995, 0.975, 0.955, 0.935, 0.915]\n",
    "#eps_decay=[0.98 ]\n",
    "dfs = pd.DataFrame()\n",
    "dfa = pd.DataFrame()\n",
    "matrix_scores = []\n",
    "matrix_avgs = []\n",
    "start_time = time.time() # Monitor Training Time  \n",
    "for i, eps in enumerate(eps_decay):\n",
    "    print('Calculation for eps_dec: ' + str(eps))\n",
    "    scores,avgs = dqn(n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=eps, \n",
    "        train_mode = True, ckpt_path='pth_checkpoints/checkpoint.pth', target_stop=False ,save_weights = False)\n",
    "    matrix_scores.append(scores)\n",
    "    matrix_avgs.append(avgs)\n",
    "print(\"\\nTotal Training time = {:.1f} min\".format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results of eps_decay analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.plot(np.arange(len(scores)), scores, label='prueba1')\n",
    "for m, eps in zip(matrix_avgs,eps_decay):\n",
    "    plt.plot(np.arange(len(m)), m, label='eps_dec: ' + str(eps))\n",
    "plt.ylabel('Moving Average (100 epsiodes)')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Train agent with DDQN and No duelling\n",
    "In this tests it is intended to get the episode in which the environment is solved, using DDQN technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = Agent(state_size= 37, action_size=4, seed=0, dueling=False, double=True)\n",
    "\n",
    "#Train agent, stop with avg target and save weights\n",
    "start_time = time.time() # Monitor Training Time  \n",
    "scores,avgs = dqn(n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.955, \n",
    "        train_mode = True, ckpt_path='pth_checkpoints/checkpoint_DDQN.pth',target_stop=True, save_weights = True) # standard values\n",
    "\n",
    "# plot the scores and the average\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label='scores')\n",
    "plt.plot(np.arange(len(scores)), avgs, color='red', label='average')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "print(\"Score: {}\".format(scores)) # Sacar la secuencia de score de los Ãºltimos 100 episodios, sobre los que se hace la media\n",
    "print(\"\\nTotal Training time = {:.1f} min\".format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Train agent with Duelling\n",
    "Train agent with No DDQN and duelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = Agent(state_size= 37, action_size=4, seed=0, dueling=True, double=True)\n",
    "\n",
    "#Train agent, stop with avg target and save weights\n",
    "start_time = time.time() # Monitor Training Time  \n",
    "scores,avgs = dqn(n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.955, \n",
    "        train_mode = True, ckpt_path='pth_checkpoints/checkpoint_Duelling.pth',target_stop=True, save_weights = True) # standard values\n",
    "\n",
    "# plot the scores and the average\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label='scores')\n",
    "plt.plot(np.arange(len(scores)), avgs, color='red', label='average')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "print(\"Score: {}\".format(scores)) # Sacar la secuencia de score de los Ãºltimos 100 episodios, sobre los que se hace la media\n",
    "print(\"\\nTotal Training time = {:.1f} min\".format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), avgs_1, color='blue', label='average_Duelling')\n",
    "plt.plot(np.arange(len(scores)), avgs_2, color='lightblue', label='average')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "print(\"Score: {}\".format(scores)) # Sacar la secuencia de score de los Ãºltimos 100 episodios, sobre los que se hace la media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Train agent with DDQN  and Duelling\n",
    "Train agent with DDQN and duelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = Agent(state_size= 37, action_size=4, seed=0, dueling=True, double=True)\n",
    "\n",
    "#Train agent, stop with avg target and save weights\n",
    "start_time = time.time() # Monitor Training Time  \n",
    "scores,avgs = dqn(n_episodes=1000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.955, \n",
    "        train_mode = True, ckpt_path='pth_checkpoints/checkpoint_DDQN_Duel.pth',target_stop=True, save_weights = True) # standard values\n",
    "\n",
    "# plot the scores and the average\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, label='scores')\n",
    "plt.plot(np.arange(len(scores)), avgs, color='red', label='average')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "print(\"Score: {}\".format(scores)) # Sacar la secuencia de score de los Ãºltimos 100 episodios, sobre los que se hace la media\n",
    "print(\"\\nTotal Training time = {:.1f} min\".format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.plot(np.arange(len(scores)), scores, label='prueba1')\n",
    "plt.plot(np.arange(len(scores)), avgs_1, color='blue', label='average')\n",
    "plt.plot(np.arange(len(scores)), avgs_2, color='lightblue', label='average')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "print(\"Score: {}\".format(scores)) # Sacar la secuencia de score de los Ãºltimos 100 episodios, sobre los que se hace la media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Load weights to see the agent performing its task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file, different status\n",
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size= 37, action_size=4, seed=0)\n",
    "\n",
    "# Load different weights trained along the session\n",
    "# Note that, if some weights are trained in GPU, it is necessary to indicate map_location='cpu' to load and evaluate in a CPU\n",
    "#agent.qnetwork_local.load_state_dict(torch.load('checkpoint40_GPU.pth', map_location='cpu')) # If trained GPU, activate map_location\n",
    "#agent.qnetwork_local.load_state_dict(torch.load('checkpoint100.pth'))\n",
    "#agent.qnetwork_local.load_state_dict(torch.load('checkpoint400.pth'))\n",
    "\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(2):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]  ##env.reset()\n",
    "    for j in range(200):\n",
    "        #action = agent.act(state)\n",
    "        action = agent.act(state)\n",
    "        action = action.astype(int)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        state = env_info.vector_observations[0] \n",
    "        #env.render()\n",
    "        done = env_info.local_done[0]\n",
    "        #state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
